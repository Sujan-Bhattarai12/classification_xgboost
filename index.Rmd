---
title: "Lab6"
author: Sujan Bhattarai
date: "2023-03-01"
output:
  pdf_document: default
  html_document: default
---

## Case Study: Eel Distribution Modeling

This week's lab follows a project modeling the eel species Anguilla australis described by Elith et al. (2008). There are two data sets for this lab. You'll use one for training and evaluating your model, and you'll use your model to make predictions predictions on the other. Then you'll compare your model's performance to the model used by Elith et al.

## Data

Grab the training and evaluation data sets (eel.model.data.csv, eel.eval.data.csv) from github here: <https://github.com/MaRo406/eds-232-machine-learning/blob/main/data>

```{r, set.seed = 123, include= FALSE}
#set all warnings and messages false and set seed for reproducibility
set.seed(123)
knitr::opts_chunk$set(warning = FALSE, message = FALSE, cache = T)

```

```{r}
#load all libraries requiref for xgboost
library(tidymodels)
library(xgboost)
library(tictoc)
library(vip)
library(readr)
library(dplyr)
library(purrr)
library(ggplot2)
library(ranger)
library(yardstick)
library(workflows)
library(recipes)
library(modeldata)
library(parsnip)
library(dials)
```


```{r}
#download the data from github using direct github link
training_data <- read_csv("https://raw.githubusercontent.com/MaRo406/eds-232-machine-learning/main/data/eel.model.data.csv")
evaluation_data <- read_csv("https://raw.githubusercontent.com/MaRo406/eds-232-machine-learning/main/data/eel.eval.data.csv")

```

```{r}
#take a look at the data, look at na values and nuermic, categorical columns
glimpse(training_data)

#conver the Angaus column to a factor
training_data$Angaus <- as.factor(training_data$Angaus)

```

### Preprocess

Create a recipe to prepare your data for the XGBoost model

```{r}
# Create a recipe to prepare your data for the XGBoost model
eel_recipe <- recipe(Angaus ~ ., data = training_data) %>%
  step_naomit(all_predictors()) %>%
  step_dummy(all_nominal(), -all_outcomes()) %>%
  step_center(all_predictors(), -all_outcomes()) %>%
  step_scale(all_predictors(), -all_outcomes())

```

### Split and Resample

Split the model data (eel.model.data.csv) into a training and test set, stratified by outcome score (Angaus). Use 10-fold CV to resample the training set.

```{r}
#split to 80-20 train and test set
set.seed(123)
eel_split <- initial_split(training_data, prop = 0.8, strata = Angaus)
train <- training(eel_split)
test <-  testing(eel_split)
```


```{r}
#use 10 fold cross validation to resample the training set
cv_set <- vfold_cv(train, v = 10, strata = Angaus)

```

## Tuning XGBoost

### Tune Learning Rate

Following the XGBoost tuning strategy outlined in lecture, first we conduct tuning on just the learning rate parameter:

1.  Create a model specification using {xgboost} for the estimation

-   Only specify one parameter to tune()

```{r}
#Instantiate the xgboost model
xgboost_spec <- boost_tree(
  learn_rate = tune())%>%
  set_engine("xgboost") %>%
  set_mode("classification")

```

2.  Set up a grid to tune your model by using a range of learning rate parameter values: expand.grid(learn_rate = seq(0.0001, 0.3, length.out = 30))

```{r}
#Set up a grid to tune your model by using a range of learning rate parameter values\
learn_rate_grid <- expand.grid(learn_rate = seq(0.0001, 0.3, length.out = 30))

```

-   Use appropriate metrics argument(s) - Computational efficiency becomes a factor as models get more complex and data get larger. Record the time it takes to run. Do this for each tuning phase you run.You could use {tictoc} or Sys.time().

```{r}
# tune the model with the learning rate grid
Sys.time()
learn_rate_tune <- tune_grid(
  xgboost_spec,
  eel_recipe,
  resamples = cv_set,
  grid = learn_rate_grid,
  metrics = metric_set(roc_auc),
  control = control_grid(save_pred = TRUE)
)

Sys.time()


```


3.  Show the performance of the best models and the estimates for the learning rate parameter values associated with each.
```{r}
#select the best model
best_learn_rate <- learn_rate_tune %>%
  select_best(metric = "roc_auc")

best_learn_rate

#show estimates in tables
learn_rate_tune %>%
  collect_metrics() %>%
  filter(.metric == "roc_auc")

# show the performance for each learning rate with the best model based on estimate vlaue
learn_rate_tune %>%
  collect_metrics() %>%
  filter(.metric == "roc_auc") %>%
  ggplot(aes(x = learn_rate, y = mean)) +
  geom_point() +
  geom_line() +
  labs(title = "Learning Rate Tuning",
       x = "Learning Rate",
       y = "ROC AUC")
```


### Tune Tree Parameters

1.  Create a new specification where you set the learning rate (which you already optimized) and tune the tree parameters.
```{r}
# tune only tree parameters
xgboost_tree_spec <- boost_tree(
  trees = tune(),
  learn_rate = best_learn_rate[[1]]) %>%
  set_engine("xgboost") %>%
  set_mode("classification")

```

2.  Set up a tuning grid. This time use grid_latin_hypercube() to get a representative sampling of the parameter space

```{r}
#Set up a tuning grid. This time use grid_latin_hypercube() to get a representative sampling of the parameter space
tree_grid <- grid_latin_hypercube(
  trees() %>% finalize(mtry = NULL),
  size = 20)

```

3.  Show the performance of the best models and the estimates for the tree parameter values associated with each.

```{r}
#use the tuning grid to tune the tree parameters
Sys.time()
tree_tune <- tune_grid(
  xgboost_tree_spec,
  eel_recipe,
  resamples = cv_set,
  grid = tree_grid,
  metrics = metric_set(roc_auc),
  control = control_grid(save_pred = TRUE)
)
Sys.time()

#show estimates in table
tree_tune %>%
  collect_metrics() %>%
  filter(.metric == "roc_auc")

#show the estimates of each tree parameter
tree_tune %>%
  collect_metrics() %>%
  filter(.metric == "roc_auc") %>%
  ggplot(aes(x = trees, y = mean)) +
  geom_point() +
  geom_line() +
  labs(title = "Tree Parameter Tuning",
       x = "Number of Trees",
       y = "ROC AUC")
```


```{r}
#select the best model 
best_tree <- tree_tune %>%
  select_best(metric = "roc_auc")

best_tree

```


### Tune Stochastic Parameters

1.  Create a new specification where you set the learning rate and tree parameters (which you already optimized) and tune the stochastic parameters.

```{r}
# tune only stochastic parameters
xgboost_stoch_spec <- boost_tree(
  mtry = tune(),
  min_n = tune(),
  sample_size = tune(),
  tree_depth = tune(),
  trees = best_tree[[1]],
  learn_rate = best_learn_rate[[1]]) %>%
  set_engine("xgboost") %>%
  set_mode("classification")

```

2.  Set up a tuning grid. Use grid_latin_hypercube() again.

```{r}
# finalize mtry range based on the number of predictors
mtry_finalized <- finalize(mtry(), train)

#tuning grid for the stochastic parameters in the xgboost model
stoch_grid <- grid_latin_hypercube(mtry_finalized,
                                   sample_size = sample_prop(),
                                   min_n(), 
                                   tree_depth(), 
                                   size = 20)

```

3.  Show the performance of the best models and the estimates for the tree parameter values associated with each.

```{r}
#use the tuning grid to tune the stochastic parameters
Sys.time()
stoch_tune <- tune_grid(
  xgboost_stoch_spec,
  eel_recipe,
  resamples = cv_set,
  grid = stoch_grid,
  metrics = metric_set(roc_auc),
  control = control_grid(save_pred = TRUE)
)

Sys.time()

#show the estimates of each stochastic parameter in table
stoch_tune %>%
  collect_metrics() %>%
  filter(.metric == "roc_auc")

#select the best model
best_stoch <- stoch_tune %>%
  select_best(metric = "roc_auc")

```

## Finalize workflow and make final prediction

```{r}
#finalize the model with best parameters
final_xgboost <- boost_tree(
  trees = best_tree[[1]],
  min_n = best_stoch[[1]],
  sample_size = best_stoch[[4]],
  tree_depth = best_stoch[[2]],
  learn_rate = best_learn_rate[[1]]) %>%
  set_engine("xgboost") %>%
  set_mode("classification")

#fit the model on the data
final_fit <- fit(final_xgboost, Angaus ~ ., train)

# use last fit to predict on the test data
eel_predictions <- last_fit(final_xgboost, 
                            Angaus ~ .,
                            eel_split) %>% 
                   collect_predictions()

```


1.  How well did your model perform? What types of errors did it make?
```{r}
#use the yardstick package to evaluate the model
eel_predictions %>%
  metrics(truth = Angaus, estimate = .pred_class)

#plot the confusion matrix as image
conf_mat <- eel_predictions %>%
  conf_mat(truth = Angaus, estimate = .pred_class)

conf_mat %>%
  autoplot(type = "heatmap")
    
```

This is one of the most misleading model accuracy I have ever worked with. I counted total number of non-Anguas in the test set and there are 160 non Anguas. and if we look at the confusion matrix, there are 151 model predictions that were correct with truth. it is not because model performed well, its becuase there were too many non-anguas values, which led to the high accuracy. I don't think this model is any better than a dummy model that randomly picks anguas values and predict it. My Kap value clearly shows that model is only 45-50% efficient.

However, we can say the model performed well with an accuracy of 0.86. The model made more false positive errors than false negative errors. The model misclassified 8 of the non-anguas as Anguas. While the model also misclassified 18 Anguas as non Anguas.

## Fit your model the evaluation data and compare performance

1.  Now used your final model to predict on the other dataset (eval.data.csv)
```{r}
#change the Angaus column to factor
evaluation_data$Angaus_obs <- as.factor(evaluation_data$Angaus_obs)

# fit with the evaluation data
final_fit <- fit(final_xgboost, Angaus_obs ~ ., evaluation_data)


#check the accuracy of the model
final_fit %>%
  predict(evaluation_data) %>%
  bind_cols(evaluation_data) %>%
  metrics(truth = Angaus_obs, estimate = .pred_class)


#plot confusion matrix
conf_mat <- final_fit %>%
  predict(evaluation_data) %>%
  bind_cols(evaluation_data) %>%
  conf_mat(truth = Angaus_obs, estimate = .pred_class)

conf_mat %>%
  autoplot(type = "heatmap")

```

2.  How does your model perform on this data?

The model performed well with this data with similar high accuracy as with training set. However, the kap accuracy is still 50%, which means this model prediction is just 50% efficient. The model made lots of error in misclasifying true anguas. It misclassified 50 of the true anguas as non-anguas. I consider this as serious false negative issue.

3.  How do your results compare to those of Elith et al.?

The model performed similar to that of Elith et al. The model used by Elith et al. had an accuracy of 0.86 for validation set, which is similar to my model. For training set, Elith had 0.95 for the training sets while my model had an accuracy of 0.85 only.

-   Use {vip} to compare variable importance

```{r}
#use vip to plot the variable importance
final_fit %>%
  vip::vip(num_features = 10)
```


-   What do your variable importance results tell you about the distribution of this eel species?

The variable of importance is in different order compared to Elith paper except for the top most variable. In my model, the most important variable is Summer Temperature, which is same as Elith paper. While the order changed for the rest of the variables. This tells me that the distribution of this eel species is highly dependent on the summer temperature. The other variables are also important but not as important as summer temperature. The changes in order can be acccounted with learning rate chosen by elith and me. He chose 0.005 and number of trees as 1000, becuase there were 1000 sites. But, I use tuning to find the best estimate for those hyperparameters. Since 1000 trees make more sense becuase there were 1000 sites, using niche knowledge is more helpful in this case and I believe Elith result is more  true represenation of the anguas species distribution.


